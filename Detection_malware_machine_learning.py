import os
# import pefile
# import pprint as pp
# import pandas as pd
# import numpy as np
# import matplotlib.pyplot as plt
# import re
# import csv
# import glob
# import hashlib
# import sys
# import struct
# import peutils
# from sklearn import model_selection
# from sklearn.metrics import classification_report
# from sklearn.metrics import confusion_matrix
# from sklearn.metrics import accuracy_score
# from sklearn.linear_model import LogisticRegression
# from sklearn.tree import DecisionTreeClassifier
# from sklearn.neighbors import KNeighborsClassifier
# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
# from sklearn.naive_bayes import GaussianNB
# from sklearn.svm import SVC
# # get sha256 of file
# def sha256_checksum(filename, block_size=65536):
#     sha256 = hashlib.sha256()
#     with open(filename, 'rb') as f:
#         for block in iter(lambda: f.read(block_size), b''):
#             sha256.update(block)
#     return sha256.hexdigest()
#
#
# class PEFile:
#     """
#     This Class is constructed by parsing the pe file for the interesting features
#     each pe file is an object by itself and we extract the needed information
#     into a dictionary
#     """
#
#     # look to add PEid signatures to detect packers
#     # https://github.com/erocarrera/pefile/blob/wiki/PEiDSignatures.md
#     # signatures = peutils.SignatureDatabase('./userdb.txt')
#
#     def __init__(self, filename):
#         self.pe = pefile.PE(filename, fast_load=True)
#         # get all PE file
#         self.filename = filename
#         self.DebugSize = self.pe.OPTIONAL_HEADER.DATA_DIRECTORY[6].Size
#         self.DebugRVA = self.pe.OPTIONAL_HEADER.DATA_DIRECTORY[6].VirtualAddress
#         self.ImageVersion = self.pe.OPTIONAL_HEADER.MajorImageVersion
#         self.OSVersion = self.pe.OPTIONAL_HEADER.MajorOperatingSystemVersion
#         self.ExportRVA = self.pe.OPTIONAL_HEADER.DATA_DIRECTORY[0].VirtualAddress
#         self.ExportSize = self.pe.OPTIONAL_HEADER.DATA_DIRECTORY[0].Size
#         self.IATRVA = self.pe.OPTIONAL_HEADER.DATA_DIRECTORY[12].VirtualAddress
#         self.ResSize = self.pe.OPTIONAL_HEADER.DATA_DIRECTORY[2].Size
#         self.LinkerVersion = self.pe.OPTIONAL_HEADER.MajorLinkerVersion
#         self.NumberOfSections = self.pe.FILE_HEADER.NumberOfSections
#         self.StackReserveSize = self.pe.OPTIONAL_HEADER.SizeOfStackReserve
#         self.Dll = self.pe.OPTIONAL_HEADER.DllCharacteristics
#         self.AddressOfEntryPoint = self.pe.OPTIONAL_HEADER.AddressOfEntryPoint
#         self.ImageBase = self.pe.OPTIONAL_HEADER.ImageBase
#
#         # If the PE file was loaded using the fast_load=True argument, we will need to parse the data directories:
#         self.pe.parse_data_directories()
#         imported_dll = {}
#         number_dll = 0
#         try:
#             for entry in self.pe.DIRECTORY_ENTRY_IMPORT:
#                 if entry is not None:
#                     # print(entry.dll)
#                     number_dll += 1
#                     for imp in entry.imports:
#                         # print('\t', hex(imp.address), imp.name)
#                         if imp.name is not None:
#                             # print(imp.name.decode())
#                             imported_dll[entry.dll.decode()] = imp.name.decode()
#         except:
#             pass  # print("[-]")
#
#         self.ImportedDLL = imported_dll
#         self.NumberOfImportDLL = number_dll
#
#         section_names = {}
#         number_sections = 0
#         try:
#             for section in self.pe.sections:
#                 number_sections += 1
#                 # print (section.Name, hex(section.VirtualAddress), hex(section.Misc_VirtualSize), section.SizeOfRawData )
#                 section_names[section.Name.decode()] = section.SizeOfRawData
#             self.SectionNames = section_names
#             self.NumberOfSections = number_sections
#         except:
#             pass  # print("[-]")
#
#         number_import_functions = 0
#         import_function = []
#
#         try:
#             if self.pe.OPTIONAL_HEADER.DATA_DIRECTORY[pefile.DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_IMPORT']].VirtualAddress != 0:
#                 self.pe.parse_data_directories(directories=[pefile.DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_IMPORT']])
#                 for entry in self.pe.DIRECTORY_ENTRY_IMPORT:
#                     for imp in entry.imports:
#                         #print('\t', hex(imp.address), imp.name)
#                         if imp.name:
#                             number_import_functions += 1
#                             import_function.append(imp.name.decode())
#         except:
#             pass#print("[-]")
#
#         self.NumberOfImportFunctions = number_import_functions
#         self.ImportedFunctions = import_function
#
#     def Construct(self):
#         sample = {}
#         #self.__dict__.items():lấy ra các trường trong dict
#         for attr, k in self.__dict__.items():
#             if(attr != "pe"):
#                 sample[attr] = k
#         return sample
#
#
# def pe2vec(directory):
#     """
#     dirty function (handling all exceptions) for each sample
#     it construct a dictionary of dictionaries in the format:
#     sample x : pe informations
#     """
#     dataset = {}
#     print("")
#     print("[*] Extracting the PE file data: ")
#     print("")
#     for subdir, dirs, files in os.walk(directory):
#         for f in files:
#             file_path = os.path.join(subdir, f)
#             try:
#
#                 pe = PEFile(file_path)
#                 dataset[str(f)] = pe.Construct()
#             except Exception as e:
#                 raise
#
#     return dataset
#
# def vec2csv(dataset, output_file):
#     df = pd.DataFrame(dataset)
#     test_data = df.transpose()  # transpose to have the features as columns and samples as rows
# # utf-8 is prefered
# #output_file = './output/dataset.csv'
#     test_data.to_csv(output_file,sep=',', encoding='utf-8')
#     print("")
#     print("[+] Saving file to: " + output_file)
#     print("")
#
#
# from sklearn.model_selection import train_test_split
#
#
# def main():
#
#     merged_df = pd.read_csv("./dataset/merged_output.csv")
#     feature_col_names = ['AddressOfEntryPoint', 'DebugRVA', 'DebugSize', 'Dll', 'ExportRVA', 'ExportSize', 'IATRVA', 'ImageBase', 'ImageVersion', 'LinkerVersion', 'NumberOfSections', 'OSVersion', 'ResSize', 'StackReserveSize', 'Malware', "NumberOfImportDLL", "NumberOfImportFunctions", "NumberOfSections" ]
#     predicted_class_names = ['Malware']
#
#     X = merged_df[feature_col_names].values     # predictor feature columns
#     y = merged_df[predicted_class_names].values # predicted class (1=true, 0=false) column
#     split_test_size = 0.30
#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split_test_size, random_state=42)
#                             # test_size = 0.3 is 30%, 42 is the answer to everything
#     c=DecisionTreeClassifier()
#     c.fit(X_train,y_train)
#
#     accu_test = np.sum(c.predict(X_test) == y_test) / float(y_test.size)
#     print("Classification accu on test ", accu_test )

from datetime import datetime
from sklearn import metrics
import pandas as pd
import hashlib
import pefile
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

import sklearn

#get sha256 in file
def sha256(filename, block_size=65536):
    sha256 = hashlib.sha256()
    with open(filename, 'rb') as f:
        for block in iter(lambda: f.read(block_size), b''):
            sha256.update(block)
    return sha256.hexdigest()

#get data in filecsv


#check sha256 have in dataset
def check_sha256_dataset(filename):
    data=pd.read_csv("./dataset/merged_output.csv")
    print(data['sha256'][2])
    for i in range(len(data)):
        if data['sha256'][i]== sha256(filename):
            return data['Malware'][i]#==1 in dataset malware if==0 in dataset clean
    return -1#not found

#get attribute of file
def get_PE(filename):
    pe = pefile.PE(filename, fast_load=True)
    pe_file={}
    pe_file['sha256']=sha256(filename)
    pe_file['DebugSize'] = pe.OPTIONAL_HEADER.DATA_DIRECTORY[6].Size
    pe_file['DebugRVA'] = pe.OPTIONAL_HEADER.DATA_DIRECTORY[6].VirtualAddress
    pe_file['ImageVersion'] = pe.OPTIONAL_HEADER.MajorImageVersion
    pe_file['OSVersion']= pe.OPTIONAL_HEADER.MajorOperatingSystemVersion
    pe_file['ExportRVA'] = pe.OPTIONAL_HEADER.DATA_DIRECTORY[0].VirtualAddress
    pe_file['ExportSize'] =pe.OPTIONAL_HEADER.DATA_DIRECTORY[0].Size
    pe_file['IATRVA'] = pe.OPTIONAL_HEADER.DATA_DIRECTORY[12].VirtualAddress
    pe_file['ResSize'] = pe.OPTIONAL_HEADER.DATA_DIRECTORY[2].Size
    pe_file['LinkerVersion'] = pe.OPTIONAL_HEADER.MajorLinkerVersion
    pe_file['NumberOfSections'] = pe.FILE_HEADER.NumberOfSections
    pe_file['StackReserveSize']= pe.OPTIONAL_HEADER.SizeOfStackReserve
    pe_file['Dll'] = pe.OPTIONAL_HEADER.DllCharacteristics
    pe_file['AddressOfEntryPoint']= pe.OPTIONAL_HEADER.AddressOfEntryPoint
    pe_file['ImageBase'] = pe.OPTIONAL_HEADER.ImageBase

    # If the PE file was loaded using the fast_load=True argument, we will need to parse the data directories:
    pe.parse_data_directories()
    imported_dll = {}
    number_dll = 0
    try:
        for entry in pe.DIRECTORY_ENTRY_IMPORT:
            if entry is not None:
                # print(entry.dll)
                number_dll += 1
                for imp in entry.imports:
                    # print('\t', hex(imp.address), imp.name)
                    if imp.name is not None:
                        # print(imp.name.decode())
                        imported_dll[entry.dll.decode()] = imp.name.decode()
    except:
        pass

    pe_file['ImportedDLL'] = imported_dll
    pe_file['NumberOfImportDLL'] = number_dll

    section_names = {}
    number_sections = 0
    try:
        for section in pe.sections:
            number_sections += 1
            # print (section.Name, hex(section.VirtualAddress), hex(section.Misc_VirtualSize), section.SizeOfRawData )
            section_names[section.Name.decode()] = section.SizeOfRawData
        pe_file['SectionNames'] = section_names
        pe_file['NumberOfSections']= number_sections
    except:
        pass  # print("[-]")

    number_import_functions = 0
    import_function = []

    try:
        if pe.OPTIONAL_HEADER.DATA_DIRECTORY[pefile.DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_IMPORT']].VirtualAddress != 0:
            pe.parse_data_directories(directories=[pefile.DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_IMPORT']])
            for entry in pe.DIRECTORY_ENTRY_IMPORT:
                for imp in entry.imports:
                    if imp.name:
                        number_import_functions += 1
                        import_function.append(imp.name.decode())
    except:
        pass

    pe_file['NumberOfImportFunctions'] = number_import_functions
    pe_file['ImportedFunctions'] = import_function
    return pe_file

def machine_learning(data,test):
    feature_col_names = ['AddressOfEntryPoint', 'DebugRVA', 'DebugSize', 'Dll', 'ExportRVA', 'ExportSize', 'IATRVA',
                         'ImageBase', 'ImageVersion', 'LinkerVersion', 'NumberOfSections', 'OSVersion', 'ResSize',
                         'StackReserveSize', "NumberOfImportDLL", "NumberOfImportFunctions",
                         "NumberOfSections"]
    predicted_class_names = ['Malware']
    X = data[feature_col_names].values  # predictor feature columns
    y = data[predicted_class_names].values  # predicted class (1=true, 0=false) column
    split_test_size = 0.3
    test1=[test]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split_test_size, random_state=42)
    rf_model = RandomForestClassifier(n_estimators=100)
    rf_model.fit(X_train, y_train.ravel())
    rf_predict_test = rf_model.predict(X_test)
    predict_test = rf_model.predict(test1)
    if(predict_test[0]==1):
        print("This is malware")
    else:
        print("file clean")
    print("Testing Accuracy: {0:.4f}".format(sklearn.metrics.accuracy_score(y_test, rf_predict_test)))
    print(predict_test)
    print("Confusion Matrix")
    print("{0}".format(metrics.confusion_matrix(y_test, rf_predict_test)))
    print()
    print("Classification Report")
    print(metrics.classification_report(y_test, rf_predict_test))

def main(filename):
    # filename = "C:/Users/damnq3198/Desktop/Machine-Learning-Malware-Detection-master/data/malware/fee18f402375b210fc7b89e29084fb8e478d5ee0f0cdb85d4618d14abb2e5197"
    # filename2 = "C:/Users/damnq3198/Desktop/Machine-Learning-Malware-Detection-master/data/clean/ffccf98379c579bc6da94188e51af360402c0fb5d0c6da72a5a12b70517e47ab"


        # Store configuration file values
        #     if (clamav(filename) == 1):
        #         print("this is malware")
        #     else:
    test = get_PE(filename)
        # test2 = get_PE(filename2)
    feature_col_names = ['AddressOfEntryPoint', 'DebugRVA', 'DebugSize', 'Dll', 'ExportRVA', 'ExportSize', 'IATRVA',
                             'ImageBase', 'ImageVersion', 'LinkerVersion', 'NumberOfSections', 'OSVersion', 'ResSize',
                             'StackReserveSize', "NumberOfImportDLL", "NumberOfImportFunctions",
                             "NumberOfSections"]
    test1 = []
    for i in range(len(feature_col_names)):
        test1.append(test[feature_col_names[i]])
    print(test1)
    data = pd.read_csv("./dataset/merged_output.csv")
        # print(data.describe())
        # print(data.isnull().values.any())
    machine_learning(data, test1)





# if __name__ == '__main__':
#
#     main()

    # test=get_PE(filename)
    # test2=get_PE(filename2)
    # feature_col_names = ['AddressOfEntryPoint', 'DebugRVA', 'DebugSize', 'Dll', 'ExportRVA', 'ExportSize', 'IATRVA',
    #                      'ImageBase', 'ImageVersion', 'LinkerVersion', 'NumberOfSections', 'OSVersion', 'ResSize',
    #                      'StackReserveSize', "NumberOfImportDLL", "NumberOfImportFunctions",
    #                      "NumberOfSections"]
    # test1=[]
    # for i in range(len(feature_col_names)):
    #     test1.append(test2[feature_col_names[i]])
    # print(test1)
    # data= pd.read_csv("./dataset/merged_output.csv")
    # # print(data.describe())
    # # print(data.isnull().values.any())
    # machine_learning(data,test1)





